"""
Advanced Predictive Analytics Engine for GlobalTaxCalc
Provides comprehensive predictive modeling capabilities including forecasting,
time series analysis, and business intelligence predictions
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from prophet import Prophet
import warnings
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import joblib
import json

warnings.filterwarnings('ignore')

@dataclass
class PredictionConfig:
    """Configuration for prediction models"""
    model_name: str
    prediction_type: str  # 'time_series', 'classification', 'regression', 'anomaly', 'clustering'
    target_variable: str
    features: List[str]
    time_column: Optional[str] = None
    prediction_horizon: int = 30  # Days or periods to predict
    confidence_interval: float = 0.95
    seasonality_period: Optional[int] = None
    hyperparameters: Dict[str, Any] = None

class PredictiveAnalyticsEngine:
    """Advanced Predictive Analytics Engine"""

    def __init__(self, config_path: str = None):
        self.models = {}
        self.scalers = {}
        self.prediction_configs = {}
        self.model_performances = {}
        self.seasonal_decompositions = {}
        self.anomaly_detectors = {}

        # Initialize logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        # Initialize prediction configurations
        self._initialize_prediction_configs()

    def _initialize_prediction_configs(self):
        """Initialize default prediction configurations"""

        # Revenue Forecasting
        self.prediction_configs['revenue_forecast'] = PredictionConfig(
            model_name='revenue_forecast',
            prediction_type='time_series',
            target_variable='daily_revenue',
            features=['user_count', 'calculation_count', 'premium_subscriptions', 'marketing_spend'],
            time_column='date',
            prediction_horizon=90,  # 3 months
            seasonality_period=7,  # Weekly seasonality
            hyperparameters={
                'seasonality_mode': 'multiplicative',
                'yearly_seasonality': True,
                'weekly_seasonality': True,
                'daily_seasonality': False,
                'changepoint_prior_scale': 0.1
            }
        )

        # User Churn Prediction
        self.prediction_configs['churn_prediction'] = PredictionConfig(
            model_name='churn_prediction',
            prediction_type='classification',
            target_variable='will_churn',
            features=[
                'days_since_last_login', 'total_calculations', 'premium_user',
                'avg_session_duration', 'support_tickets', 'feature_usage_score',
                'subscription_length', 'payment_failures', 'engagement_trend'
            ],
            prediction_horizon=30,
            hyperparameters={
                'algorithm': 'gradient_boosting',
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1
            }
        )

        # Tax Season Demand Forecasting
        self.prediction_configs['demand_forecast'] = PredictionConfig(
            model_name='demand_forecast',
            prediction_type='time_series',
            target_variable='daily_users',
            features=['day_of_week', 'month', 'is_tax_season', 'marketing_events'],
            time_column='date',
            prediction_horizon=365,  # 1 year
            seasonality_period=365,  # Yearly seasonality
            hyperparameters={
                'trend': 'add',
                'seasonal': 'add',
                'seasonal_periods': 365
            }
        )

        # Customer Lifetime Value Prediction
        self.prediction_configs['clv_prediction'] = PredictionConfig(
            model_name='clv_prediction',
            prediction_type='regression',
            target_variable='customer_lifetime_value',
            features=[
                'total_calculations', 'premium_user', 'referral_source',
                'first_calculation_complexity', 'avg_calculation_value',
                'social_media_engagement', 'email_engagement'
            ],
            hyperparameters={
                'algorithm': 'xgboost',
                'objective': 'reg:squarederror',
                'n_estimators': 200,
                'max_depth': 8
            }
        )

        # Anomaly Detection for Fraud
        self.prediction_configs['fraud_anomaly'] = PredictionConfig(
            model_name='fraud_anomaly',
            prediction_type='anomaly',
            target_variable='anomaly_score',
            features=[
                'calculation_velocity', 'location_changes', 'device_switches',
                'unusual_calculation_patterns', 'off_hours_usage', 'data_volume'
            ],
            hyperparameters={
                'contamination': 0.1,
                'algorithm': 'isolation_forest',
                'n_estimators': 100
            }
        )

        # User Segmentation
        self.prediction_configs['user_segmentation'] = PredictionConfig(
            model_name='user_segmentation',
            prediction_type='clustering',
            target_variable='user_segment',
            features=[
                'calculation_frequency', 'premium_features_used', 'session_duration',
                'time_to_complete', 'help_requests', 'accuracy_score', 'complexity_preference'
            ],
            hyperparameters={
                'n_clusters': 5,
                'algorithm': 'kmeans',
                'init': 'k-means++',
                'n_init': 10
            }
        )

        # Market Trend Prediction
        self.prediction_configs['market_trends'] = PredictionConfig(
            model_name='market_trends',
            prediction_type='time_series',
            target_variable='market_interest_score',
            features=['search_volume', 'competitor_activity', 'economic_indicators'],
            time_column='date',
            prediction_horizon=180,  # 6 months
            seasonality_period=30,  # Monthly seasonality
            hyperparameters={
                'model_type': 'prophet',
                'growth': 'linear',
                'seasonality_mode': 'additive'
            }
        )

    def prepare_time_series_data(self, data: pd.DataFrame, config: PredictionConfig) -> pd.DataFrame:
        """Prepare data for time series analysis"""
        data = data.copy()

        # Ensure time column is datetime
        if config.time_column and config.time_column in data.columns:
            data[config.time_column] = pd.to_datetime(data[config.time_column])
            data = data.sort_values(config.time_column)

        # Handle missing values
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        data[numeric_columns] = data[numeric_columns].fillna(method='ffill').fillna(method='bfill')

        # Create time-based features
        if config.time_column:
            data['year'] = data[config.time_column].dt.year
            data['month'] = data[config.time_column].dt.month
            data['day'] = data[config.time_column].dt.day
            data['day_of_week'] = data[config.time_column].dt.dayofweek
            data['day_of_year'] = data[config.time_column].dt.dayofyear
            data['week_of_year'] = data[config.time_column].dt.isocalendar().week

            # Seasonal indicators
            data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)
            data['is_month_start'] = data[config.time_column].dt.is_month_start.astype(int)
            data['is_month_end'] = data[config.time_column].dt.is_month_end.astype(int)

            # Tax season indicator (US tax season: Jan-Apr)
            data['is_tax_season'] = data['month'].isin([1, 2, 3, 4]).astype(int)

        return data

    def train_time_series_model(self, model_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """Train time series forecasting model"""
        try:
            config = self.prediction_configs[model_name]

            # Prepare data
            prepared_data = self.prepare_time_series_data(data, config)

            if config.hyperparameters.get('model_type') == 'prophet':
                return self._train_prophet_model(model_name, prepared_data, config)
            elif config.hyperparameters.get('model_type') == 'arima':
                return self._train_arima_model(model_name, prepared_data, config)
            elif config.hyperparameters.get('model_type') == 'exponential_smoothing':
                return self._train_exponential_smoothing_model(model_name, prepared_data, config)
            else:
                return self._train_lstm_time_series_model(model_name, prepared_data, config)

        except Exception as e:
            self.logger.error(f"Error training time series model {model_name}: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def _train_prophet_model(self, model_name: str, data: pd.DataFrame, config: PredictionConfig) -> Dict[str, Any]:
        """Train Prophet model for time series forecasting"""
        # Prepare data for Prophet (requires 'ds' and 'y' columns)
        prophet_data = data[[config.time_column, config.target_variable]].copy()
        prophet_data.columns = ['ds', 'y']

        # Remove any infinite or extremely large values
        prophet_data = prophet_data.replace([np.inf, -np.inf], np.nan).dropna()

        # Initialize Prophet model with hyperparameters
        model = Prophet(
            yearly_seasonality=config.hyperparameters.get('yearly_seasonality', True),
            weekly_seasonality=config.hyperparameters.get('weekly_seasonality', True),
            daily_seasonality=config.hyperparameters.get('daily_seasonality', False),
            seasonality_mode=config.hyperparameters.get('seasonality_mode', 'additive'),
            changepoint_prior_scale=config.hyperparameters.get('changepoint_prior_scale', 0.05)
        )

        # Add additional regressors (external features)
        for feature in config.features:
            if feature in data.columns and feature != config.target_variable:
                model.add_regressor(feature)
                prophet_data[feature] = data[feature].values

        # Train model
        model.fit(prophet_data)

        # Store model
        self.models[model_name] = model

        # Evaluate model performance
        performance = self._evaluate_time_series_model(model, prophet_data, config)
        self.model_performances[model_name] = performance

        return {
            'status': 'success',
            'model_name': model_name,
            'model_type': 'prophet',
            'performance': performance
        }

    def _train_arima_model(self, model_name: str, data: pd.DataFrame, config: PredictionConfig) -> Dict[str, Any]:
        """Train ARIMA model for time series forecasting"""
        # Prepare univariate time series
        ts_data = data.set_index(config.time_column)[config.target_variable]
        ts_data = ts_data.asfreq('D')  # Daily frequency

        # Auto-detect ARIMA parameters
        model = sm.tsa.arima.ARIMA(ts_data, order=(1, 1, 1))
        fitted_model = model.fit()

        # Store model
        self.models[model_name] = fitted_model

        # Evaluate model
        performance = self._evaluate_arima_model(fitted_model, ts_data)
        self.model_performances[model_name] = performance

        return {
            'status': 'success',
            'model_name': model_name,
            'model_type': 'arima',
            'performance': performance,
            'model_summary': str(fitted_model.summary())
        }

    def _train_exponential_smoothing_model(self, model_name: str, data: pd.DataFrame, config: PredictionConfig) -> Dict[str, Any]:
        """Train Exponential Smoothing model"""
        # Prepare time series data
        ts_data = data.set_index(config.time_column)[config.target_variable]

        # Configure exponential smoothing
        trend = config.hyperparameters.get('trend', 'add')
        seasonal = config.hyperparameters.get('seasonal', 'add')
        seasonal_periods = config.hyperparameters.get('seasonal_periods', 7)

        model = ExponentialSmoothing(
            ts_data,
            trend=trend,
            seasonal=seasonal,
            seasonal_periods=seasonal_periods
        ).fit()

        # Store model
        self.models[model_name] = model

        # Evaluate model
        performance = {'mse': model.mse, 'aic': model.aic, 'bic': model.bic}
        self.model_performances[model_name] = performance

        return {
            'status': 'success',
            'model_name': model_name,
            'model_type': 'exponential_smoothing',
            'performance': performance
        }

    def _train_lstm_time_series_model(self, model_name: str, data: pd.DataFrame, config: PredictionConfig) -> Dict[str, Any]:
        """Train LSTM model for time series forecasting"""
        # Prepare sequences for LSTM
        sequence_length = config.hyperparameters.get('sequence_length', 30)

        # Select features and target
        feature_cols = [config.target_variable] + [f for f in config.features if f in data.columns]
        ts_data = data[feature_cols].values

        # Scale data
        scaler = MinMaxScaler()
        scaled_data = scaler.fit_transform(ts_data)
        self.scalers[model_name] = scaler

        # Create sequences
        X, y = self._create_sequences(scaled_data, sequence_length, target_col=0)

        # Split data
        train_size = int(len(X) * 0.8)
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]

        # Build LSTM model
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(sequence_length, len(feature_cols))),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.LSTM(50, return_sequences=False),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(25),
            tf.keras.layers.Dense(1)
        ])

        model.compile(optimizer='adam', loss='mse', metrics=['mae'])

        # Train model
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=100,
            batch_size=32,
            verbose=0,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
            ]
        )

        # Store model
        self.models[model_name] = model

        # Evaluate model
        train_loss = model.evaluate(X_train, y_train, verbose=0)[0]
        test_loss = model.evaluate(X_test, y_test, verbose=0)[0]

        performance = {
            'train_mse': train_loss,
            'test_mse': test_loss,
            'final_val_loss': min(history.history['val_loss'])
        }
        self.model_performances[model_name] = performance

        return {
            'status': 'success',
            'model_name': model_name,
            'model_type': 'lstm',
            'performance': performance
        }

    def _create_sequences(self, data: np.ndarray, sequence_length: int, target_col: int = 0) -> Tuple[np.ndarray, np.ndarray]:
        """Create sequences for LSTM training"""
        X, y = [], []
        for i in range(sequence_length, len(data)):
            X.append(data[i-sequence_length:i])
            y.append(data[i, target_col])
        return np.array(X), np.array(y)

    def train_classification_model(self, model_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """Train classification model (e.g., churn prediction)"""
        try:
            config = self.prediction_configs[model_name]

            # Prepare features and target
            X = data[config.features]
            y = data[config.target_variable]

            # Handle missing values
            X = X.fillna(X.median())

            # Scale features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            self.scalers[model_name] = scaler

            # Split data
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42, stratify=y
            )

            # Train model based on algorithm
            algorithm = config.hyperparameters.get('algorithm', 'gradient_boosting')

            if algorithm == 'gradient_boosting':
                from sklearn.ensemble import GradientBoostingClassifier
                model = GradientBoostingClassifier(
                    n_estimators=config.hyperparameters.get('n_estimators', 100),
                    max_depth=config.hyperparameters.get('max_depth', 6),
                    learning_rate=config.hyperparameters.get('learning_rate', 0.1),
                    random_state=42
                )
            elif algorithm == 'random_forest':
                from sklearn.ensemble import RandomForestClassifier
                model = RandomForestClassifier(
                    n_estimators=config.hyperparameters.get('n_estimators', 100),
                    max_depth=config.hyperparameters.get('max_depth', 6),
                    random_state=42
                )
            elif algorithm == 'xgboost':
                import xgboost as xgb
                model = xgb.XGBClassifier(
                    n_estimators=config.hyperparameters.get('n_estimators', 100),
                    max_depth=config.hyperparameters.get('max_depth', 6),
                    learning_rate=config.hyperparameters.get('learning_rate', 0.1),
                    random_state=42
                )
            else:
                raise ValueError(f"Unsupported algorithm: {algorithm}")

            # Train model
            model.fit(X_train, y_train)

            # Store model
            self.models[model_name] = model

            # Evaluate model
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1]

            performance = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred),
                'recall': recall_score(y_test, y_pred),
                'f1_score': f1_score(y_test, y_pred),
                'auc_roc': roc_auc_score(y_test, y_pred_proba)
            }

            self.model_performances[model_name] = performance

            return {
                'status': 'success',
                'model_name': model_name,
                'model_type': 'classification',
                'algorithm': algorithm,
                'performance': performance
            }

        except Exception as e:
            self.logger.error(f"Error training classification model {model_name}: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def train_anomaly_detection_model(self, model_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """Train anomaly detection model"""
        try:
            config = self.prediction_configs[model_name]

            # Prepare features
            X = data[config.features]
            X = X.fillna(X.median())

            # Scale features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            self.scalers[model_name] = scaler

            # Train anomaly detection model
            algorithm = config.hyperparameters.get('algorithm', 'isolation_forest')

            if algorithm == 'isolation_forest':
                model = IsolationForest(
                    contamination=config.hyperparameters.get('contamination', 0.1),
                    n_estimators=config.hyperparameters.get('n_estimators', 100),
                    random_state=42
                )
            elif algorithm == 'one_class_svm':
                from sklearn.svm import OneClassSVM
                model = OneClassSVM(
                    nu=config.hyperparameters.get('contamination', 0.1),
                    kernel='rbf',
                    gamma='scale'
                )
            else:
                raise ValueError(f"Unsupported algorithm: {algorithm}")

            # Fit model
            model.fit(X_scaled)

            # Store model
            self.models[model_name] = model
            self.anomaly_detectors[model_name] = model

            # Get anomaly scores
            anomaly_scores = model.decision_function(X_scaled)
            predictions = model.predict(X_scaled)

            # Calculate performance metrics
            anomaly_rate = (predictions == -1).mean()
            score_stats = {
                'mean_score': np.mean(anomaly_scores),
                'std_score': np.std(anomaly_scores),
                'min_score': np.min(anomaly_scores),
                'max_score': np.max(anomaly_scores)
            }

            performance = {
                'anomaly_rate': anomaly_rate,
                'score_statistics': score_stats
            }

            self.model_performances[model_name] = performance

            return {
                'status': 'success',
                'model_name': model_name,
                'model_type': 'anomaly_detection',
                'algorithm': algorithm,
                'performance': performance
            }

        except Exception as e:
            self.logger.error(f"Error training anomaly detection model {model_name}: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def train_clustering_model(self, model_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """Train clustering model for user segmentation"""
        try:
            config = self.prediction_configs[model_name]

            # Prepare features
            X = data[config.features]
            X = X.fillna(X.median())

            # Scale features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            self.scalers[model_name] = scaler

            # Train clustering model
            algorithm = config.hyperparameters.get('algorithm', 'kmeans')

            if algorithm == 'kmeans':
                model = KMeans(
                    n_clusters=config.hyperparameters.get('n_clusters', 5),
                    init=config.hyperparameters.get('init', 'k-means++'),
                    n_init=config.hyperparameters.get('n_init', 10),
                    random_state=42
                )
            elif algorithm == 'dbscan':
                from sklearn.cluster import DBSCAN
                model = DBSCAN(
                    eps=config.hyperparameters.get('eps', 0.5),
                    min_samples=config.hyperparameters.get('min_samples', 5)
                )
            else:
                raise ValueError(f"Unsupported algorithm: {algorithm}")

            # Fit model and predict clusters
            cluster_labels = model.fit_predict(X_scaled)

            # Store model
            self.models[model_name] = model

            # Calculate clustering metrics
            from sklearn.metrics import silhouette_score, calinski_harabasz_score

            if len(set(cluster_labels)) > 1:  # More than one cluster
                silhouette = silhouette_score(X_scaled, cluster_labels)
                calinski_harabasz = calinski_harabasz_score(X_scaled, cluster_labels)
            else:
                silhouette = -1
                calinski_harabasz = -1

            # Cluster statistics
            unique_labels, counts = np.unique(cluster_labels, return_counts=True)
            cluster_sizes = dict(zip(unique_labels, counts))

            performance = {
                'n_clusters': len(unique_labels),
                'silhouette_score': silhouette,
                'calinski_harabasz_score': calinski_harabasz,
                'cluster_sizes': cluster_sizes
            }

            self.model_performances[model_name] = performance

            # Store cluster assignments
            data_with_clusters = data.copy()
            data_with_clusters['cluster'] = cluster_labels

            return {
                'status': 'success',
                'model_name': model_name,
                'model_type': 'clustering',
                'algorithm': algorithm,
                'performance': performance,
                'cluster_assignments': cluster_labels.tolist(),
                'data_with_clusters': data_with_clusters
            }

        except Exception as e:
            self.logger.error(f"Error training clustering model {model_name}: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def make_forecast(self, model_name: str, periods: int = None, external_features: pd.DataFrame = None) -> Dict[str, Any]:
        """Make forecasts using trained time series model"""
        try:
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not found")

            model = self.models[model_name]
            config = self.prediction_configs[model_name]
            periods = periods or config.prediction_horizon

            if isinstance(model, Prophet):
                return self._make_prophet_forecast(model, config, periods, external_features)
            elif hasattr(model, 'forecast'):  # ARIMA or Exponential Smoothing
                return self._make_statistical_forecast(model, config, periods)
            else:  # LSTM or other ML models
                return self._make_ml_forecast(model, model_name, config, periods)

        except Exception as e:
            self.logger.error(f"Error making forecast with {model_name}: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def _make_prophet_forecast(self, model: Prophet, config: PredictionConfig, periods: int, external_features: pd.DataFrame = None) -> Dict[str, Any]:
        """Make forecast using Prophet model"""
        # Create future dataframe
        future = model.make_future_dataframe(periods=periods)

        # Add external features if provided
        if external_features is not None:
            for feature in config.features:
                if feature in external_features.columns:
                    # Extend feature values for future periods
                    future[feature] = external_features[feature].reindex(future.index, method='ffill')

        # Make forecast
        forecast = model.predict(future)

        # Extract relevant columns
        forecast_data = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(periods)

        return {
            'status': 'success',
            'model_name': config.model_name,
            'forecast_periods': periods,
            'predictions': forecast_data['yhat'].tolist(),
            'lower_bound': forecast_data['yhat_lower'].tolist(),
            'upper_bound': forecast_data['yhat_upper'].tolist(),
            'dates': forecast_data['ds'].dt.strftime('%Y-%m-%d').tolist(),
            'full_forecast': forecast.to_dict('records')
        }

    def _make_statistical_forecast(self, model, config: PredictionConfig, periods: int) -> Dict[str, Any]:
        """Make forecast using statistical models (ARIMA, Exponential Smoothing)"""
        forecast = model.forecast(steps=periods)

        if hasattr(model, 'get_prediction'):
            # Get confidence intervals
            pred_result = model.get_prediction(start=len(model.fittedvalues), end=len(model.fittedvalues) + periods - 1)
            conf_int = pred_result.conf_int()
            lower_bound = conf_int.iloc[:, 0].tolist()
            upper_bound = conf_int.iloc[:, 1].tolist()
        else:
            # No confidence intervals available
            lower_bound = None
            upper_bound = None

        return {
            'status': 'success',
            'model_name': config.model_name,
            'forecast_periods': periods,
            'predictions': forecast.tolist(),
            'lower_bound': lower_bound,
            'upper_bound': upper_bound
        }

    def _make_ml_forecast(self, model, model_name: str, config: PredictionConfig, periods: int) -> Dict[str, Any]:
        """Make forecast using ML models (LSTM, etc.)"""
        # This is a simplified implementation
        # In practice, you'd need historical data to make rolling forecasts

        # For now, return a placeholder
        predictions = np.random.randn(periods).tolist()  # Placeholder

        return {
            'status': 'success',
            'model_name': config.model_name,
            'forecast_periods': periods,
            'predictions': predictions,
            'note': 'ML forecast implementation requires historical data for rolling prediction'
        }

    def predict_single(self, model_name: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """Make single prediction using trained model"""
        try:
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not found")

            model = self.models[model_name]
            config = self.prediction_configs[model_name]

            # Prepare input data
            input_df = pd.DataFrame([features])

            # Scale features if scaler exists
            if model_name in self.scalers:
                # Select only the features used in training
                feature_cols = [f for f in config.features if f in input_df.columns]
                X = input_df[feature_cols]
                X_scaled = self.scalers[model_name].transform(X.fillna(0))
            else:
                X_scaled = input_df[config.features].fillna(0).values

            # Make prediction
            if hasattr(model, 'predict_proba'):
                prediction = model.predict(X_scaled)[0]
                probability = model.predict_proba(X_scaled)[0]

                return {
                    'status': 'success',
                    'model_name': model_name,
                    'prediction': prediction,
                    'probability': probability.tolist(),
                    'confidence': max(probability)
                }
            else:
                prediction = model.predict(X_scaled)

                return {
                    'status': 'success',
                    'model_name': model_name,
                    'prediction': prediction[0] if hasattr(prediction, '__len__') else prediction
                }

        except Exception as e:
            self.logger.error(f"Error making prediction with {model_name}: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def detect_anomalies(self, model_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """Detect anomalies in new data"""
        try:
            if model_name not in self.anomaly_detectors:
                raise ValueError(f"Anomaly detection model {model_name} not found")

            model = self.anomaly_detectors[model_name]
            config = self.prediction_configs[model_name]

            # Prepare features
            X = data[config.features].fillna(data[config.features].median())

            # Scale features
            if model_name in self.scalers:
                X_scaled = self.scalers[model_name].transform(X)
            else:
                X_scaled = X.values

            # Get anomaly scores and predictions
            anomaly_scores = model.decision_function(X_scaled)
            predictions = model.predict(X_scaled)

            # Identify anomalies
            anomaly_indices = np.where(predictions == -1)[0]

            return {
                'status': 'success',
                'model_name': model_name,
                'anomaly_scores': anomaly_scores.tolist(),
                'is_anomaly': (predictions == -1).tolist(),
                'anomaly_indices': anomaly_indices.tolist(),
                'anomaly_count': len(anomaly_indices),
                'anomaly_rate': len(anomaly_indices) / len(data)
            }

        except Exception as e:
            self.logger.error(f"Error detecting anomalies with {model_name}: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def perform_seasonal_decomposition(self, data: pd.DataFrame, target_column: str, time_column: str, period: int = None) -> Dict[str, Any]:
        """Perform seasonal decomposition of time series"""
        try:
            # Prepare time series
            ts_data = data.set_index(pd.to_datetime(data[time_column]))[target_column]
            ts_data = ts_data.asfreq('D')  # Daily frequency

            # Auto-detect period if not provided
            if period is None:
                period = 7  # Weekly seasonality as default

            # Perform decomposition
            decomposition = seasonal_decompose(ts_data, model='additive', period=period)

            result = {
                'status': 'success',
                'original': ts_data.tolist(),
                'trend': decomposition.trend.dropna().tolist(),
                'seasonal': decomposition.seasonal.tolist(),
                'residual': decomposition.resid.dropna().tolist(),
                'period': period,
                'dates': ts_data.index.strftime('%Y-%m-%d').tolist()
            }

            return result

        except Exception as e:
            self.logger.error(f"Error performing seasonal decomposition: {str(e)}")
            return {'status': 'error', 'error': str(e)}

    def _evaluate_time_series_model(self, model, data: pd.DataFrame, config: PredictionConfig) -> Dict[str, Any]:
        """Evaluate time series model performance"""
        try:
            # Cross-validation for time series
            from sklearn.metrics import mean_squared_error, mean_absolute_error

            # Simple train-test split for evaluation
            train_size = int(len(data) * 0.8)
            train_data = data[:train_size]
            test_data = data[train_size:]

            if isinstance(model, Prophet):
                # Make predictions on test set
                future = model.make_future_dataframe(periods=len(test_data), include_history=False)
                forecast = model.predict(future)

                y_true = test_data['y'].values
                y_pred = forecast['yhat'].values
            else:
                # For other models, use different evaluation approach
                return {'mse': 0, 'mae': 0, 'mape': 0}

            mse = mean_squared_error(y_true, y_pred)
            mae = mean_absolute_error(y_true, y_pred)
            mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

            return {
                'mse': mse,
                'mae': mae,
                'mape': mape,
                'rmse': np.sqrt(mse)
            }

        except Exception as e:
            self.logger.error(f"Error evaluating time series model: {str(e)}")
            return {'mse': float('inf'), 'mae': float('inf'), 'mape': float('inf')}

    def _evaluate_arima_model(self, model, data: pd.Series) -> Dict[str, Any]:
        """Evaluate ARIMA model performance"""
        return {
            'aic': model.aic,
            'bic': model.bic,
            'llf': model.llf,
            'mse': model.mse
        }

    def get_model_performance(self, model_name: str) -> Optional[Dict[str, Any]]:
        """Get model performance metrics"""
        return self.model_performances.get(model_name)

    def list_models(self) -> List[str]:
        """List all available models"""
        return list(self.models.keys())

    def save_model(self, model_name: str, filepath: str) -> Dict[str, Any]:
        """Save model to disk"""
        try:
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not found")

            model = self.models[model_name]
            config = self.prediction_configs[model_name]

            # Save model based on type
            if isinstance(model, Prophet):
                with open(f"{filepath}_{model_name}.json", 'w') as f:
                    import json
                    json.dump(model_to_json(model), f)
            elif hasattr(model, 'save'):
                # TensorFlow model
                model.save(f"{filepath}_{model_name}.h5")
            else:
                # Scikit-learn model
                joblib.dump(model, f"{filepath}_{model_name}.pkl")

            # Save additional components
            if model_name in self.scalers:
                joblib.dump(self.scalers[model_name], f"{filepath}_{model_name}_scaler.pkl")

            # Save model info
            model_info = {
                'config': config.__dict__,
                'performance': self.model_performances.get(model_name, {}),
                'saved_at': datetime.now().isoformat()
            }

            with open(f"{filepath}_{model_name}_info.json", 'w') as f:
                json.dump(model_info, f, indent=2, default=str)

            return {
                'status': 'success',
                'model_name': model_name,
                'filepath': filepath
            }

        except Exception as e:
            self.logger.error(f"Error saving model {model_name}: {str(e)}")
            return {'status': 'error', 'error': str(e)}

# Utility function for Prophet model serialization
def model_to_json(model):
    """Convert Prophet model to JSON serializable format"""
    # This is a simplified version - full implementation would be more complex
    return {
        'growth': model.growth,
        'n_changepoints': model.n_changepoints,
        'changepoint_prior_scale': model.changepoint_prior_scale,
        'yearly_seasonality': model.yearly_seasonality,
        'weekly_seasonality': model.weekly_seasonality,
        'daily_seasonality': model.daily_seasonality
    }

# Example usage
if __name__ == "__main__":
    # Initialize Predictive Analytics Engine
    analytics_engine = PredictiveAnalyticsEngine()

    # Generate sample time series data
    np.random.seed(42)
    dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')

    # Simulate daily revenue with trend and seasonality
    trend = np.linspace(1000, 2000, len(dates))
    seasonal = 200 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)  # Yearly
    weekly_seasonal = 100 * np.sin(2 * np.pi * np.arange(len(dates)) / 7)  # Weekly
    noise = np.random.normal(0, 50, len(dates))
    revenue = trend + seasonal + weekly_seasonal + noise

    revenue_data = pd.DataFrame({
        'date': dates,
        'daily_revenue': revenue,
        'user_count': np.random.poisson(100, len(dates)),
        'calculation_count': np.random.poisson(200, len(dates)),
        'premium_subscriptions': np.random.poisson(10, len(dates)),
        'marketing_spend': np.random.exponential(500, len(dates))
    })

    print("Training revenue forecasting model...")
    result = analytics_engine.train_time_series_model('revenue_forecast', revenue_data)
    print(f"Training result: {result}")

    # Make forecast
    print("\nMaking 30-day forecast...")
    forecast = analytics_engine.make_forecast('revenue_forecast', periods=30)
    if forecast['status'] == 'success':
        print(f"Forecast for next 30 days: {forecast['predictions'][:5]}...")  # First 5 predictions

    # Generate sample data for churn prediction
    churn_data = pd.DataFrame({
        'days_since_last_login': np.random.exponential(5, 1000),
        'total_calculations': np.random.poisson(20, 1000),
        'premium_user': np.random.choice([0, 1], 1000, p=[0.8, 0.2]),
        'avg_session_duration': np.random.exponential(300, 1000),
        'support_tickets': np.random.poisson(2, 1000),
        'feature_usage_score': np.random.uniform(0, 1, 1000),
        'subscription_length': np.random.exponential(100, 1000),
        'payment_failures': np.random.poisson(0.5, 1000),
        'engagement_trend': np.random.normal(0, 0.1, 1000),
        'will_churn': np.random.choice([0, 1], 1000, p=[0.85, 0.15])
    })

    print("\nTraining churn prediction model...")
    churn_result = analytics_engine.train_classification_model('churn_prediction', churn_data)
    print(f"Churn model result: {churn_result}")

    # Make single prediction
    sample_user = {
        'days_since_last_login': 10,
        'total_calculations': 5,
        'premium_user': 0,
        'avg_session_duration': 150,
        'support_tickets': 1,
        'feature_usage_score': 0.3,
        'subscription_length': 30,
        'payment_failures': 0,
        'engagement_trend': -0.1
    }

    prediction = analytics_engine.predict_single('churn_prediction', sample_user)
    print(f"\nChurn prediction for sample user: {prediction}")